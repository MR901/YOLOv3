{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.draw\n",
    "from matplotlib import pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "from mrcnn import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = 'chicken-nuggets'\n",
    "root = 'root'\n",
    "prefix = 'data'\n",
    "prefix_1 = 'output'\n",
    "prefix_2 = 'logs'\n",
    "prefix_3 = 'weights'\n",
    " \n",
    "\n",
    "data = 's3://{}/{}'.format(bucket, prefix)\n",
    "logs = 's3://{}/{}'.format(bucket, prefix_2)\n",
    "weights = 's3://{}/()'.format(bucket, prefix_3)\n",
    "root = 's3://{}/{}'.format(bucket, root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChickenConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"chicken\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 2\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 2  # Background + objects\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChickenDataset(utils.Dataset):\n",
    "    def load_VIA(self, dataset_dir, subset, hc=False):\n",
    "        \"\"\"Load the surgery dataset from VIA.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val or predict\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"chicken\", 1, \"blackcrumb\")\n",
    "        self.add_class(\"chicken\", 2, \"redcrumb\")\n",
    "        if hc is True:\n",
    "            for i in range(1,14):\n",
    "                self.add_class(\"chicken\", i, \"{}\".format(i))\n",
    "            self.add_class(\"chicken\", 14, \"arm\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\", \"predict\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # VGG Image Annotator saves each image in the form:\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {name:'a'},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, \"via_region_data.json\")))\n",
    "\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. There are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            polygons = [r['shape_attributes'] for r in a['regions']]\n",
    "            names = [r['region_attributes'] for r in a['regions']]\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"chicken\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons,\n",
    "                names=names)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a surgery dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"chicken\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        class_names = info[\"names\"]\n",
    "        #print(class_names)\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "        # Assign class_ids by reading class_names\n",
    "        class_ids = np.zeros([len(info[\"polygons\"])])\n",
    "        # In the surgery dataset, pictures are labeled with name 'a' and 'r' representing arm and ring.\n",
    "        for i, p in enumerate(class_names):\n",
    "        #\"name\" is the attributes name decided when labeling, etc. 'region_attributes': {name:'a'}\n",
    "            if p['name'] == 'blackcrumb':\n",
    "                class_ids[i] = 1\n",
    "            elif p['name'] == 'redcrumb':\n",
    "                class_ids[i] = 2\n",
    "            #assert code here to extend to other labels\n",
    "        class_ids = class_ids.astype(int)\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask.astype(np.bool), class_ids\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"chicken\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n",
    "    def load_mask_hc(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a surgery dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"chicken\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        #\"name\" is the attributes name decided when labeling, etc. 'region_attributes': {name:'a'}\n",
    "        class_names = info[\"names\"]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "        # Assign class_ids by reading class_names\n",
    "        class_ids = np.zeros([len(info[\"polygons\"])])\n",
    "        # In the surgery dataset, pictures are labeled with name 'a' and 'r' representing arm and ring.\n",
    "        for i, p in enumerate(class_names):\n",
    "            if p['name'] == 'blackcrumb':\n",
    "                class_ids[i] = 14\n",
    "            elif p['name'] == 'error':\n",
    "                pass\n",
    "            else:\n",
    "                class_ids[i] = int(p['name'])\n",
    "            #assert code here to extend to other labels\n",
    "        class_ids = class_ids.astype(int)\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask.astype(np.bool), class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, *dic):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = ChickenDataset()\n",
    "    dataset_train.load_VIA(data, \"train\")\n",
    "    dataset_train.prepare()\n",
    "    \n",
    "    augmentation = iaa.Sometimes(.667, iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    # Small gaussian blur with random sigma between 0 and 0.25.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.25))\n",
    "    ),\n",
    "    iaa.Dropout([0.05, 0.1]),\n",
    "    iaa.ElasticTransformation(alpha=50, sigma=5),\n",
    "    # Strengthen or weaken the contrast in each image.\n",
    "    iaa.ContrastNormalization((0.75, 1.5)),\n",
    "    # Add gaussian noise.\n",
    "    # For 50% of all images, we sample the noise once per pixel.\n",
    "    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "    # channel. This can change the color (not only brightness) of the\n",
    "    # pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255)),\n",
    "    # Make some images brighter and some darker.\n",
    "    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2)),\n",
    "    # Apply affine transformations to each image.\n",
    "    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        #translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-180, 180),\n",
    "        #shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True)) # apply augmenters in random order\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = ChickenDataset()\n",
    "    dataset_val.load_VIA(args.dataset, \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # *** This training schedu le is an example. Update to your needs ***\n",
    "    # Since we're using a very small dataset, and starting from\n",
    "    # COCO trained weights, we don't need to train too long. Also,\n",
    "    # no need to train all layers, just the heads should do it.\n",
    "    print(\"Training network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=100,\n",
    "                layers='heads',\n",
    "               augmentation=augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(mask):\n",
    "    \"\"\"Encodes a mask in Run Length Encoding (RLE).\n",
    "    Returns a string of space-separated values.\n",
    "    \"\"\"\n",
    "    assert mask.ndim == 2, \"Mask must be of shape [Height, Width]\"\n",
    "    # Flatten it column wise\n",
    "    m = mask.T.flatten()\n",
    "    # Compute gradient. Equals 1 or -1 at transition points\n",
    "    g = np.diff(np.concatenate([[0], m, [0]]), n=1)\n",
    "    # 1-based indicies of transition points (where gradient != 0)\n",
    "    rle = np.where(g != 0)[0].reshape([-1, 2]) + 1\n",
    "    # Convert second index in each pair to lenth\n",
    "    rle[:, 1] = rle[:, 1] - rle[:, 0]\n",
    "    return \" \".join(map(str, rle.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(rle, shape):\n",
    "    \"\"\"Decodes an RLE encoded list of space separated\n",
    "    numbers and returns a binary mask.\"\"\"\n",
    "    rle = list(map(int, rle.split()))\n",
    "    rle = np.array(rle, dtype=np.int32).reshape([-1, 2])\n",
    "    rle[:, 1] += rle[:, 0]\n",
    "    rle -= 1\n",
    "    mask = np.zeros([shape[0] * shape[1]], np.bool)\n",
    "    for s, e in rle:\n",
    "        assert 0 <= s < mask.shape[0]\n",
    "        assert 1 <= e <= mask.shape[0], \"shape: {}  s {}  e {}\".format(shape, s, e)\n",
    "        mask[s:e] = 1\n",
    "    # Reshape and transpose\n",
    "    mask = mask.reshape([shape[1], shape[0]]).T\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(image_id, mask, scores):\n",
    "    \"Encodes instance masks to submission format.\"\n",
    "    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\n",
    "    # If mask is empty, return line with image ID only\n",
    "    if mask.shape[-1] == 0:\n",
    "        return \"{},\".format(image_id)\n",
    "    # Remove mask overlaps\n",
    "    # Multiply each instance mask by its score order\n",
    "    # then take the maximum across the last dimension\n",
    "    order = np.argsort(scores)[::-1] + 1  # 1-based descending\n",
    "    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\n",
    "    # Loop over instance masks\n",
    "    lines = []\n",
    "    for o in order:\n",
    "        m = np.where(mask == o, 1, 0)\n",
    "        # Skip if empty\n",
    "        if m.sum() == 0.0:\n",
    "            continue\n",
    "        rle = rle_encode(m)\n",
    "        lines.append(\"{}, {}\".format(image_id, rle))\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    command = \"train\"\n",
    "    config = \"ChickenConfig\"\n",
    "    print(\"Weights: \", args.weights)\n",
    "    print(\"Dataset: \", args.dataset)\n",
    "    print(\"Logs: \", args.logs)\n",
    "    \n",
    "    model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=logs)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
